{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf9ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "\n",
    "# # model_study ka absolute path leke sys.path mein daal\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# from model_study.LMEDR import LMEDRModel\n",
    "\n",
    "# print(\"Import done ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef0f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model_study.LMEDR import LMEDRModel\n",
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e988f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec0ee6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "   \"num_latent\": 10,\n",
    "   \"num_latent2\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7fdb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92a0d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully ✅\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "add_special_tokens = {'additional_special_tokens': ['<query>', '<response>', '<latent>', '<persona>']}\n",
    "tokenizer.add_special_tokens(add_special_tokens)\n",
    "# model = LMEDRModel.from_pretrained(\"facebook/bart-large\", num_labels=1,\n",
    "#                                            num_token=len(tokenizer),\n",
    "#                                            num_latent=args.num_latent, num_latent2=args.num_latent2)\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids('<response>')\n",
    "# model.config.forced_bos_token_id = None\n",
    "print(\"Model loaded successfully ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5048907",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from = \"_original\"\n",
    "args.max_history = 5\n",
    "args.cand = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1178ddc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/ConvAI2/valid_self_original.txt has 1000 dialog and 7801 query\n",
      "Checkpoint: Data for training created.\n",
      "Checkpoint: Training dataloader built.\n",
      "./data/ConvAI2/valid_self_original.txt has 1000 dialog and 7801 query\n",
      "Checkpoint: Data for validation created.\n",
      "Checkpoint: Validation dataloader built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate infer data: 100%|██████████| 310110/310110 [00:38<00:00, 8098.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: Inference dataset built.\n"
     ]
    }
   ],
   "source": [
    "from build_data_PersonaChat import create_data,build_dataloader,build_infer_dataset\n",
    "\n",
    "persona, query, response, cand = create_data(f\"./data/ConvAI2/valid_self{data_from}.txt\")\n",
    "print(\"Checkpoint: Data for training created.\")\n",
    "train_data = build_dataloader(persona, query, response, cand, tokenizer, max_history=args.max_history, n_cand=args.cand)\n",
    "print(\"Checkpoint: Training dataloader built.\")\n",
    "persona, query, response, cand = create_data(f\"./data/ConvAI2/valid_self{data_from}.txt\")\n",
    "print(\"Checkpoint: Data for validation created.\")\n",
    "val_data = build_dataloader(persona, query, response, cand, tokenizer, max_history=args.max_history, use_all=True)\n",
    "print(\"Checkpoint: Validation dataloader built.\")\n",
    "infer_data = build_infer_dataset(tokenizer, \"./data/dnli/dialogue_nli_train.jsonl\")\n",
    "print(\"Checkpoint: Inference dataset built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a10325cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([7801, 5, 246])\n",
      "input_ids: torch.Size([7801, 20, 246])\n",
      "attention_mask: torch.Size([7801, 5, 246])\n",
      "attention_mask: torch.Size([7801, 20, 246])\n",
      "lmlabels: torch.Size([7801, 5, 30])\n",
      "lmlabels: torch.Size([7801, 20, 30])\n",
      "decoder_input_ids: torch.Size([7801, 5, 30])\n",
      "decoder_input_ids: torch.Size([7801, 20, 30])\n",
      "decoder_attention_mask: torch.Size([7801, 5, 30])\n",
      "decoder_attention_mask: torch.Size([7801, 20, 30])\n",
      "cls_index: torch.Size([7801, 5, 30])\n",
      "cls_index: torch.Size([7801, 20, 30])\n",
      "clslabel: torch.Size([7801])\n",
      "clslabel: torch.Size([7801])\n",
      "per_input_ids: torch.Size([7801, 5, 69])\n",
      "per_input_ids: torch.Size([7801, 20, 69])\n",
      "per_attention_mask: torch.Size([7801, 5, 69])\n",
      "per_attention_mask: torch.Size([7801, 20, 69])\n",
      "encoder_input_ids: torch.Size([100000, 1, 62])\n",
      "decoder_input_ids: torch.Size([100000, 1, 42])\n",
      "attention_mask: torch.Size([100000, 1, 62])\n",
      "decoder_attention_mask: torch.Size([100000, 1, 42])\n",
      "lmlabels: torch.Size([100000, 1, 42])\n"
     ]
    }
   ],
   "source": [
    "MODEL_INPUTS = [\"input_ids\", \"attention_mask\", \"lmlabels\", \"decoder_input_ids\", \"decoder_attention_mask\",\n",
    "                \"cls_index\", \"clslabel\", \"per_input_ids\", \"per_attention_mask\"]\n",
    "\n",
    "INFER_INPUTS = [\"encoder_input_ids\", \"decoder_input_ids\", \"attention_mask\", \"decoder_attention_mask\",\n",
    "                \"lmlabels\"]\n",
    "\n",
    "trainset = []\n",
    "valset = []\n",
    "inferset = []\n",
    "\n",
    "for input_name in MODEL_INPUTS:\n",
    "    if input_name == \"clslabel\":\n",
    "        tensor = train_data[input_name].view(-1)\n",
    "        print(\"{}: {}\".format(input_name, tensor.size()))\n",
    "        trainset.append(tensor)\n",
    "\n",
    "        tensor = val_data[input_name].view(-1)\n",
    "        print(\"{}: {}\".format(input_name, tensor.size()))\n",
    "        valset.append(tensor)\n",
    "    else:\n",
    "        tensor = train_data[input_name].view(-1, args.cand, train_data[input_name].size(-1))\n",
    "        print(\"{}: {}\".format(input_name, tensor.size()))\n",
    "        trainset.append(tensor)\n",
    "\n",
    "        tensor = val_data[input_name].view(-1, 20, val_data[input_name].size(-1))\n",
    "        print(\"{}: {}\".format(input_name, tensor.size()))\n",
    "        valset.append(tensor)\n",
    "\n",
    "for input_name in INFER_INPUTS:\n",
    "    tensor = infer_data[input_name].view(-1, 1, infer_data[input_name].size(-1))\n",
    "    print(\"{}: {}\".format(input_name, tensor.size()))\n",
    "    inferset.append(tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fca830",
   "metadata": {},
   "source": [
    "### Study the MODEL_INPUTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0868c84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39005"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b7bf380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([39005, 246])\n",
      "attention_mask torch.Size([39005, 246])\n",
      "per_input_ids torch.Size([39005, 69])\n",
      "per_attention_mask torch.Size([39005, 69])\n",
      "lmlabels torch.Size([39005, 30])\n",
      "decoder_input_ids torch.Size([39005, 30])\n",
      "decoder_attention_mask torch.Size([39005, 30])\n",
      "cls_index torch.Size([39005, 30])\n",
      "clslabel torch.Size([7801, 1])\n"
     ]
    }
   ],
   "source": [
    "for key in train_data.keys():\n",
    "    if key != 0:\n",
    "       print(key,train_data[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88e5d527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:torch.Size([246])\n",
      "attention_mask:torch.Size([246])\n",
      "per_input_ids:torch.Size([69])\n",
      "per_attention_mask:torch.Size([69])\n",
      "lmlabels:torch.Size([30])\n",
      "decoder_input_ids:torch.Size([30])\n",
      "decoder_attention_mask:torch.Size([30])\n",
      "cls_index:torch.Size([30])\n",
      "clslabel:torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for key in train_data.keys():\n",
    "    if key != 0:\n",
    "        # print(f\"Exploring {key}\")\n",
    "        # PRinting Single Instance of Each data\n",
    "        for data in train_data[key]:\n",
    "            print(f\"{key}:{data.shape}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dab0dc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:tensor([50267, 50268,   939,  1166, 10328,  2799,    10,    76,     4,     2,\n",
      "          939,   437,    10, 16391,  1457,    25,   127,   200,   633,     4,\n",
      "            2,   939,   129,  3529, 36930,     4,     2,   939,    21,  1179,\n",
      "           11,    10,   881,  4095,  6028,     4,     2, 50265, 20760,    99,\n",
      "           32,   608,   452, 17487,     2,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1])\n",
      "attention_mask:tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "per_input_ids:tensor([50267, 50268,   939,  1166, 10328,  2799,    10,    76,     4,     2,\n",
      "          939,   437,    10, 16391,  1457,    25,   127,   200,   633,     4,\n",
      "            2,   939,   129,  3529, 36930,     4,     2,   939,    21,  1179,\n",
      "           11,    10,   881,  4095,  6028,     4,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "per_attention_mask:tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "lmlabels:tensor([ 939,  524,  205, 2156,  939,   95,  300,  160,  173,    8, 7428, 2156,\n",
      "         939,   33,   80, 1315,  479,    2, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "decoder_input_ids:tensor([50266,   939,   524,   205,  2156,   939,    95,   300,   160,   173,\n",
      "            8,  7428,  2156,   939,    33,    80,  1315,   479,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "decoder_attention_mask:tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "cls_index:tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100,    2, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "clslabel:tensor([0])\n"
     ]
    }
   ],
   "source": [
    "for key in train_data.keys():\n",
    "    if key != 0:\n",
    "        # print(f\"Exploring {key}\")\n",
    "        # PRinting Single Instance of Each data\n",
    "        for data in train_data[key]:\n",
    "            print(f\"{key}:{data}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0932fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "key = \"per_input_ids\"\n",
    "for i,data in enumerate(train_data[key]):\n",
    "    print(tokenizer.decode(data))\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "212c384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s> <query>  hello what are doing today ?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s> <query>  hello what are doing today ?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s> <query>  hello what are doing today ?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s> <query>  hello what are doing today ?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s> <query>  hello what are doing today ?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s> <query>  hello what are doing today ?</s> <response>  i am good , i just got off work and tired , i have two jobs .</s> <query>  i just got done watching a horror movie</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s> <query>  hello what are doing today ?</s> <response>  i am good , i just got off work and tired , i have two jobs .</s> <query>  i just got done watching a horror movie</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s> <query>  hello what are doing today ?</s> <response>  i am good , i just got off work and tired , i have two jobs .</s> <query>  i just got done watching a horror movie</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s> <query>  hello what are doing today ?</s> <response>  i am good , i just got off work and tired , i have two jobs .</s> <query>  i just got done watching a horror movie</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s> <query>  hello what are doing today ?</s> <response>  i am good , i just got off work and tired , i have two jobs .</s> <query>  i just got done watching a horror movie</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s> <query>  hello what are doing today ?</s> <response>  i am good , i just got off work and tired , i have two jobs .</s> <query>  i just got done watching a horror movie</s> <response>  i rather read , i've read about 20 books this year .</s> <query>  wow ! i do love a good horror movie . loving this cooler weather</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "key = \"input_ids\"\n",
    "for i,data in enumerate(train_data[key]):\n",
    "    print(tokenizer.decode(data))\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8ba822b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' i am good , i just got off work and tired , i have two jobs .</s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([939,  524,  205, 2156,  939,   95,  300,  160,  173,    8, 7428, 2156,\n",
    "         939,   33,   80, 1315,  479,    2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8765898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 939,  524,  205, 2156,  939,   95,  300,  160,  173,    8, 7428, 2156,\n",
      "         939,   33,   80, 1315,  479,    2, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([ 939, 1195, 1166, 2156,  939,  348, 1166,   59,  291, 2799,   42,   76,\n",
      "         479,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([  53,   10,  205, 1569,   16,  460,  205,  479,    2, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "key = \"lmlabels\"\n",
    "for i,data in enumerate(train_data[key]):\n",
    "    print(data)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66c1f808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<response>  i am good , i just got off work and tired , i have two jobs .<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<response>  yeah , well what about you ?<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<response>  hey there , are you a mother ?<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<response>  i don't like clowns . they are scary to a kid like me<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<response>  it sure is . i'd like to see more of the city though .<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<response>  i rather read , i've read about 20 books this year .<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<response>  like me ? would you go on vacation to the beach with me<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<response>  sounds like a good plan , what would you like to teach ?<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<response>  why have you not sent help ? ! the scorpions are stinging my legs ! ree ! ! ! ! ! ! !<pad><pad><pad>\n",
      "<response>  where do you work ?<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<response>  but a good movie is always good .<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "key = \"decoder_input_ids\"\n",
    "for i,data in enumerate(train_data[key]):\n",
    "    print(tokenizer.decode(data))\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe1241fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100,    2, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100,    2, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100,    2, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100,    2, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100,    2, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100,    2, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100,    2, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100,    2, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "key =\"cls_index\"\n",
    "for i,data in enumerate(train_data[key]):\n",
    "    print(data)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e758f185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<latent> <persona>  i read twenty books a year.</s> i'm a stunt double as my second job.</s> i only eat kosher.</s> i was raised in a single parent household.</s> <query>  hello what are doing today ?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_data[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a78c0fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7801, 5, 246])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb97b80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7801, 5, 246]),\n",
       " torch.Size([7801, 5, 246]),\n",
       " torch.Size([7801, 5, 30]),\n",
       " torch.Size([7801, 5, 30]),\n",
       " torch.Size([7801, 5, 30]),\n",
       " torch.Size([7801, 5, 30]),\n",
       " torch.Size([7801]),\n",
       " torch.Size([7801, 5, 69]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0].shape, trainset[1].shape, trainset[2].shape, trainset[3].shape, trainset[4].shape, trainset[5].shape, trainset[6].shape, trainset[7].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e66d856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valset saved to 'valset.pkl'\n",
      "inferset saved to 'inferset.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# # Save the trainset to a pickle file\n",
    "# with open('trainset.pkl', 'wb') as f_train:\n",
    "#     pickle.dump(trainset, f_train)\n",
    "\n",
    "# print(\"trainset saved to 'trainset.pkl'\")\n",
    "\n",
    "# Save the valset to a pickle file\n",
    "with open('valset.pkl', 'wb') as f_val:\n",
    "    pickle.dump(valset, f_val)\n",
    "\n",
    "print(\"valset saved to 'valset.pkl'\")\n",
    "\n",
    "# Save the inferset to a pickle file\n",
    "with open('inferset.pkl', 'wb') as f_infer:\n",
    "    pickle.dump(inferset, f_infer)\n",
    "\n",
    "print(\"inferset saved to 'inferset.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29e07a",
   "metadata": {},
   "source": [
    "### INFER_INPUTS dataset Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFER_INPUTS = [\"encoder_input_ids\", \"decoder_input_ids\", \"attention_mask\", \"decoder_attention_mask\",\"lmlabels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b95f24c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_ids: tensor([50267, 50268,   939,   524,   129,   820,    98,   939,    74,    45,\n",
      "          216,   479,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "<latent> <persona>  i am only 22 so i would not know .</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "decoder_input_ids: tensor([50266,   939,   524, 10328,    80,   107,   793,   479,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "<response>  i am twenty two years old .<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "lmlabels: tensor([  939,   524, 10328,    80,   107,   793,   479,     2,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "for key, data in zip(INFER_INPUTS,inferset):\n",
    "    # print(f\"{key}: {data.shape}\")\n",
    "    if key in [\"encoder_input_ids\", \"decoder_input_ids\"]:\n",
    "       print(f\"{key}: {data[0].squeeze(0)}\")\n",
    "       print(tokenizer.decode(data[0].squeeze(0)))\n",
    "    elif key ==\"lmlabels\":\n",
    "       print(f\"{key}: {data[0].squeeze(0)}\")\n",
    "    #    print(tokenizer.decode(data[0].squeeze(0)))\n",
    "    else:\n",
    "        print(data[0].squeeze(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f00397c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' i am twenty two years old .</s>'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([939,   524, 10328,    80,   107,   793,   479,     2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eca973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3779a537",
   "metadata": {},
   "source": [
    "## Temp area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This Lines Taking too much memory with train dataset\n",
    "\n",
    "# persona, query, response, cand = create_data(f\"../data/ConvAI2/train_self{data_from}.txt\")\n",
    "# print(\"Checkpoint: Data for training created.\")\n",
    "# train_data = build_dataloader(persona, query, response, cand, tokenizer, max_history=args.max_history, n_cand=args.cand)\n",
    "# print(\"Checkpoint: Training dataloader built.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d2e9d",
   "metadata": {},
   "source": [
    "### Temp Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f93216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import logging\n",
    "\n",
    "args.distributed = False  # Set to True if using distributed training\n",
    "args.train_batch_size = 2\n",
    "args.valid_batch_size = 2\n",
    "args.infer_batch_size = 32\n",
    "\n",
    "# Initialize logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Assuming trainset, valset, and inferset are already defined and are tuples of tensors\n",
    "train_dataset = TensorDataset(*trainset)\n",
    "val_dataset = TensorDataset(*valset)\n",
    "infer_dataset = TensorDataset(*inferset)\n",
    "\n",
    "logger.info(\"Prepare dataloader.\")\n",
    "\n",
    "# Set up DistributedSampler for distributed training, if applicable\n",
    "train_sampler = DistributedSampler(train_dataset) if args.distributed else None\n",
    "infer_sampler = DistributedSampler(infer_dataset) if args.distributed else None\n",
    "val_sampler = DistributedSampler(val_dataset) if args.distributed else None\n",
    "\n",
    "# Create DataLoader for training, inference, and validation\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    sampler=train_sampler, \n",
    "    batch_size=args.train_batch_size,\n",
    "    shuffle=(not args.distributed), \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "infer_loader = DataLoader(\n",
    "    infer_dataset, \n",
    "    sampler=infer_sampler, \n",
    "    batch_size=args.infer_batch_size,\n",
    "    shuffle=(not args.distributed), \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    sampler=val_sampler, \n",
    "    batch_size=args.valid_batch_size, \n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70bf898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader Batch Shapes:\n",
      "torch.Size([2, 5, 246])\n",
      "torch.Size([2, 5, 246])\n",
      "torch.Size([2, 5, 30])\n",
      "torch.Size([2, 5, 30])\n",
      "torch.Size([2, 5, 30])\n",
      "torch.Size([2, 5, 30])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 5, 69])\n",
      "torch.Size([2, 5, 69])\n",
      "\n",
      "Validation Loader Batch Shapes:\n",
      "torch.Size([2, 20, 246])\n",
      "torch.Size([2, 20, 246])\n",
      "torch.Size([2, 20, 30])\n",
      "torch.Size([2, 20, 30])\n",
      "torch.Size([2, 20, 30])\n",
      "torch.Size([2, 20, 30])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 20, 69])\n",
      "torch.Size([2, 20, 69])\n",
      "\n",
      "Infer Loader Batch Shapes:\n",
      "torch.Size([32, 1, 62])\n",
      "torch.Size([32, 1, 42])\n",
      "torch.Size([32, 1, 62])\n",
      "torch.Size([32, 1, 42])\n",
      "torch.Size([32, 1, 42])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(\"Train Loader Batch Shapes:\")\n",
    "    for tensor in data:\n",
    "        print(tensor.shape)\n",
    "    break\n",
    "\n",
    "for data in val_loader:\n",
    "    print(\"\\nValidation Loader Batch Shapes:\")\n",
    "    for tensor in data:\n",
    "        print(tensor.shape)\n",
    "    break\n",
    "\n",
    "for data in infer_loader:\n",
    "    print(\"\\nInfer Loader Batch Shapes:\")\n",
    "    for tensor in data:\n",
    "        print(tensor.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccf6afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "args.lr = 5e-5\n",
    "\n",
    "memory1_params = list(map(id, model.memory1))\n",
    "base_params = filter(lambda p: id(p) not in memory1_params, model.parameters())\n",
    "\n",
    "optimizer_infer = AdamW(model.parameters(), lr=args.lr, correct_bias=True)\n",
    "optimizer_bart = AdamW(base_params, lr=args.lr, correct_bias=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d7a6c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "args.max_grad_norm = 1.0\n",
    "args.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def infer_update(engine, batch):\n",
    "        model.train()\n",
    "        batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
    "        infer_input_ids, infer_decoder_input_ids, infer_attention_mask, \\\n",
    "        infer_decoder_attention_mask, infer_lmlabels = batch\n",
    "        outputs = model(infer_input_ids=infer_input_ids,\n",
    "                        infer_decoder_input_ids=infer_decoder_input_ids,\n",
    "                        infer_attention_mask=infer_attention_mask,\n",
    "                        infer_lmlabels=infer_lmlabels,\n",
    "                        infer_decoder_attention_mask=infer_decoder_attention_mask\n",
    "                        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "        optimizer_infer.step()\n",
    "        optimizer_infer.zero_grad()\n",
    "        return {'loss': loss.item()}\n",
    "\n",
    "infer_trainer = Engine(infer_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b0d1d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "args.epochs = 1\n",
    "\n",
    "@infer_trainer.on(Events.STARTED)\n",
    "def log_training_start(engine):\n",
    "    print(\"Training started...\")\n",
    "\n",
    "@infer_trainer.on(Events.COMPLETED)\n",
    "def log_training_end(engine):\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "@infer_trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    print(f\"Epoch {engine.state.epoch} - Loss: {engine.state.output['loss']:.4f}\")\n",
    "\n",
    "@infer_trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    if engine.state.iteration % 100 == 0:\n",
    "        print(f\"Iteration {engine.state.iteration} - Loss: {engine.state.output['loss']:.4f}\")\n",
    "\n",
    "infer_trainer.run(infer_loader, max_epochs=args.epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmedr_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
