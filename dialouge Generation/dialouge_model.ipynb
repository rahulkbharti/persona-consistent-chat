{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3127fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da8aaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the BART model and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "add_special_tokens = {'additional_special_tokens': ['<query>', '<response>', '<latent>', '<persona>']}\n",
    "tokenizer.add_special_tokens(add_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c3f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "   \"num_latent\": 10,\n",
    "   \"num_latent2\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1b355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tokens = tokenizer.encode(add_special_tokens['additional_special_tokens'])\n",
    "args['bos'], args[\"query\"], args[\"response\"], args[\"latent\"], args[\"persona\"], args[\"eos\"] = encoded_tokens\n",
    "args[\"pad\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e9377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartScaledWordEmbedding(50269, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the updated tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5be94c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50269, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50269, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50269, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50269, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee822d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [50267, 50268, 100, 95, 2162, 10, 1518, 92, 790, 4, 2, 100, 101, 7, 3836, 23, 5, 950, 4, 2, 100, 422, 10, 2335, 41227, 334, 4, 2, 100, 33, 10, 380, 4045, 13495, 4, 2, 100, 101, 602, 8, 6016, 842, 462, 16731, 4, 2, 50265, 30086, 6, 38, 437, 646, 44518, 112, 18, 766, 8174, 653, 18, 110, 766, 116, 2], 'labels': [50266, 30086, 6, 38, 437, 646, 44518, 132, 18, 766, 8174, 85, 18, 2579, 7, 972, 47, 4, 2]}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load the pickle file\n",
    "with open('./Synthetic-Persona-Chat/prepared_data/test_data.pkl', 'rb') as f:\n",
    "    data_set = pickle.load(f)\n",
    "\n",
    "# Now `data_set` contains your loaded data\n",
    "print(data_set[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "157e6302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50266, 30086,     6,    38,   437,   646, 44518,   132,    18,   766,\n",
       "         8174,    85,    18,  2579,     7,   972,    47,     4,     2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(data_set[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a037761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to torch dataset\n",
    "class CNNDailyMailDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids = torch.tensor(item['input_ids'])\n",
    "        # attention_mask = torch.tensor(item['attention_mask'])\n",
    "        labels = torch.tensor(item['labels'])\n",
    "         \n",
    "\n",
    "        decoder_input_ids = labels\n",
    "        # labels = labels[1:]\n",
    "\n",
    "        # decoder_input_ids = labels[:-1]\n",
    "        # labels = labels[1:]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            # 'attention_mask': attention_mask,\n",
    "            'decoder_input_ids': decoder_input_ids,\n",
    "            'labels': labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5aa3ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CollateFn:\n",
    "    def __init__(self, pad_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        input_ids = [item['input_ids'] for item in batch]\n",
    "        decoder_input_ids = [item['decoder_input_ids'] for item in batch]\n",
    "        labels = [item['labels'] for item in batch]\n",
    "\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id)\n",
    "        decoder_input_ids = pad_sequence(decoder_input_ids, batch_first=True, padding_value=self.pad_token_id)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # ignore index\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'decoder_input_ids': decoder_input_ids,\n",
    "            'labels': labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b483dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CNNDailyMailDataset(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02b292c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = args['pad']\n",
    "\n",
    "pad_fn = CollateFn(pad_token_id)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=2,collate_fn=pad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66c6506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID: tensor([[50267, 50268,   100,   173,    25,    10,  3254,     4,     2,   100,\n",
      "           524,    10,  2602, 37958,     4,     2,   100,   524,  2997,    19,\n",
      "            10,  1159,     4,     2,   100,   657,     7,  7142,     4,     2,\n",
      "           100,   101,  2600,     4,     2, 50265,  1711,    18,   205,     7,\n",
      "          1798,     4,     2, 50266,  2847,     6,    99,    18,   110,  2674,\n",
      "           689,     7,  7142,   116,     2, 50265,   100,   657,  6836, 18236,\n",
      "             4,    38,   146,    10,  1266, 33362,   741,  8982,  4977,   242,\n",
      "             4,     2, 50266,  7516,     6,    14,  4428, 10964,     4,    38,\n",
      "           657, 18236,   350,     4,     2, 50265,  5096,   350,     4,    38,\n",
      "           115,  3529,    24,   358,   183,     4,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [50267, 50268,   100,   657,     5,  1971,  6187,     4,     2, 10285,\n",
      "           688,   939,    21,    23,    10,   964,  3312,     4,     2,  2387,\n",
      "         21039,    16, 11267,  1521,     4,     2,   100,   173,    11,   647,\n",
      "             4,     2, 50265,  1185,   197,   328,    85,    18,    10,   269,\n",
      "           205,   527,    12,  9756,   177,     4,     2, 50266,   100,   581,\n",
      "            33,     7,  1649,    24,    66,     4,   653,    59,  4133,   116,\n",
      "           653,    18,   110,  2674,  1569,   116,     2, 50265,   100,   101,\n",
      "            10,   319,     9,   430,  4133,     6,    53,   127,  2674,    16,\n",
      "          1153,    20,  8390,  1193,  3153, 34279,     4,     2, 50266,  1711,\n",
      "            18,    10,   372,  1569,   328,    38,   657,     5,   527,     4,\n",
      "             2, 50265, 14783,     6,    24,    18,    10,   269,   205,  1569,\n",
      "             4,     2]])\n",
      "Decoder Input IDs : tensor([[50266,   100,   437,    45,   686,    38,   115,  3679,    14,     4,\n",
      "            38,   657, 18236,     6,    53,    38,   218,    75,   206,    38,\n",
      "           115,  3529,    24,   358,   183,     4,     2],\n",
      "        [50266,   243,    18,  2579,     7,   972,   951,    54,    34,  1122,\n",
      "          3168,     7,   162,     4,     2,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]])\n",
      "Labels: tensor([[50266,   100,   437,    45,   686,    38,   115,  3679,    14,     4,\n",
      "            38,   657, 18236,     6,    53,    38,   218,    75,   206,    38,\n",
      "           115,  3529,    24,   358,   183,     4,     2],\n",
      "        [50266,   243,    18,  2579,     7,   972,   951,    54,    34,  1122,\n",
      "          3168,     7,   162,     4,     2,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    # Move the batch to the device\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    decoder_input_ids = batch['decoder_input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    # Print the shapes of the tensors\n",
    "    print(f\"Input ID: {input_ids}\")\n",
    "    print(f\"Decoder Input IDs : {decoder_input_ids}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af50efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# num_epochs = 1  # for example\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\")\n",
    "\n",
    "#     for batch_idx, batch in progress_bar:\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "\n",
    "#         outputs = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Calculate perplexity for the current batch\n",
    "#         perplexity = torch.exp(loss).item()\n",
    "\n",
    "#         # Update progress bar description\n",
    "#         progress_bar.set_postfix({\n",
    "#             'Batch': batch_idx+1,\n",
    "#             'Loss': f\"{loss.item():.4f}\",\n",
    "#             'Perplexity': f\"{perplexity:.4f}\"\n",
    "#         })\n",
    "\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     print(f\"Epoch {epoch+1} completed | Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c51ca86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully from ../model.pth\n"
     ]
    }
   ],
   "source": [
    "load_path = \"../model.pth\"\n",
    "model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Model loaded successfully from\", load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e6bf846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:    I love hiking and reading.  I also love to spend time with my family and friends.\n"
     ]
    }
   ],
   "source": [
    "query = \"<latent><persona> I love hiking and books. <query> What's your hobby?\"\n",
    "inputs = tokenizer(query, return_tensors='pt').to(device)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    max_length=50,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Response:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71512911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_persona(model, tokenizer, device):\n",
    "    print(\"Start chatting! Type 'exit' to stop.\\n\")\n",
    "\n",
    "    # Persona definition\n",
    "    persona_lines = [\n",
    "        \"I love playing video games. </s>\",\n",
    "        \"Hey there, my name is Sidhartha and I am a veterinarian. </s>\",\n",
    "        \"I am also a musician on the weekends. </s>\",\n",
    "        \"Love to read drama books. </s>\"\n",
    "    ]\n",
    "    persona = \" \".join(persona_lines)\n",
    "\n",
    "    # Initialize context with persona\n",
    "    context = f\"<persona> {persona}</s>\"\n",
    "\n",
    "    # Store conversation history\n",
    "    conversation_history = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        print(\"You:\",user_input)\n",
    "        print(\"Thinking...\")\n",
    "        # Add user input to conversation history\n",
    "        conversation_history.append(f\"<query> {user_input}</s>\")\n",
    "        context += f\" <query> {user_input}</s>\"\n",
    "\n",
    "        # Keep the context limited to 5 exchanges (max)\n",
    "        if len(conversation_history) > 5:\n",
    "            # Pop the first exchange (oldest query-response pair)\n",
    "            context = context.replace(conversation_history[0], \"\", 1)\n",
    "            conversation_history.pop(0)\n",
    "\n",
    "        # Tokenize and send to model\n",
    "        inputs = tokenizer(context, return_tensors='pt', truncation=True).to(device)\n",
    "\n",
    "        # Generate response\n",
    "        output_ids = model.generate(\n",
    "           input_ids=inputs['input_ids'],\n",
    "           max_length=200,\n",
    "           do_sample=True,           # for sampling-based generation\n",
    "           temperature=0.7,          # controls randomness\n",
    "           top_p=0.7,                # nucleus sampling\n",
    "           top_k=50,                 # top-k sampling\n",
    "           num_return_sequences=1    # number of generated responses\n",
    "        )\n",
    "\n",
    "\n",
    "        # Decode response\n",
    "        response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "        # Print and add bot response to context\n",
    "        \n",
    "        print(\"Bot:\", response)\n",
    "        conversation_history.append(f\"<response> {response}</s>\")\n",
    "        context += f\" <response> {response}</s>\"\n",
    "        print(\"_\" * 50)\n",
    "        # print(\"Context:\", context)\n",
    "        # print(\"_\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871d543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting! Type 'exit' to stop.\n",
      "\n",
      "You: what is your feverite game\n",
      "Thinking...\n",
      "Bot: .  I'm Siddharth.  What do you like to do for fun?\n",
      "__________________________________________________\n",
      "You: jg\n",
      "Thinking...\n"
     ]
    }
   ],
   "source": [
    "chat_with_persona(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96c40a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmedr_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
