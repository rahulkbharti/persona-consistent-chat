{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3127fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da8aaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the BART model and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "add_special_tokens = {'additional_special_tokens': ['<query>', '<response>', '<latent>', '<persona>']}\n",
    "tokenizer.add_special_tokens(add_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0c3f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "   \"num_latent\": 10,\n",
    "   \"num_latent2\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a1b355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tokens = tokenizer.encode(add_special_tokens['additional_special_tokens'])\n",
    "args['bos'], args[\"query\"], args[\"response\"], args[\"latent\"], args[\"persona\"], args[\"eos\"] = encoded_tokens\n",
    "args[\"pad\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e9377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartScaledWordEmbedding(50269, 768, padding_idx=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the updated tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5be94c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50269, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50269, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50269, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50269, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee822d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [50267, 50268, 100, 95, 2162, 10, 1518, 92, 790, 4, 2, 100, 101, 7, 3836, 23, 5, 950, 4, 2, 100, 422, 10, 2335, 41227, 334, 4, 2, 100, 33, 10, 380, 4045, 13495, 4, 2, 100, 101, 602, 8, 6016, 842, 462, 16731, 4, 2, 50265, 30086, 6, 38, 437, 646, 44518, 112, 18, 766, 8174, 653, 18, 110, 766, 116, 2], 'labels': [50266, 30086, 6, 38, 437, 646, 44518, 132, 18, 766, 8174, 85, 18, 2579, 7, 972, 47, 4, 2]}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load the pickle file\n",
    "with open('./Synthetic-Persona-Chat/prepared_data/test_data.pkl', 'rb') as f:\n",
    "    data_set = pickle.load(f)\n",
    "\n",
    "# Now `data_set` contains your loaded data\n",
    "print(data_set[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "157e6302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50266, 30086,     6,    38,   437,   646, 44518,   132,    18,   766,\n",
       "         8174,    85,    18,  2579,     7,   972,    47,     4,     2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(data_set[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a037761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to torch dataset\n",
    "class CNNDailyMailDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids = torch.tensor(item['input_ids'])\n",
    "        # attention_mask = torch.tensor(item['attention_mask'])\n",
    "        labels = torch.tensor(item['labels'])\n",
    "         \n",
    "\n",
    "        decoder_input_ids = labels\n",
    "        # labels = labels[1:]\n",
    "\n",
    "        # decoder_input_ids = labels[:-1]\n",
    "        # labels = labels[1:]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            # 'attention_mask': attention_mask,\n",
    "            'decoder_input_ids': decoder_input_ids,\n",
    "            'labels': labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5aa3ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CollateFn:\n",
    "    def __init__(self, pad_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        input_ids = [item['input_ids'] for item in batch]\n",
    "        decoder_input_ids = [item['decoder_input_ids'] for item in batch]\n",
    "        labels = [item['labels'] for item in batch]\n",
    "\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id)\n",
    "        decoder_input_ids = pad_sequence(decoder_input_ids, batch_first=True, padding_value=self.pad_token_id)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # ignore index\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'decoder_input_ids': decoder_input_ids,\n",
    "            'labels': labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b483dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CNNDailyMailDataset(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02b292c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = args['pad']\n",
    "\n",
    "pad_fn = CollateFn(pad_token_id)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=2,collate_fn=pad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66c6506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID: tensor([[50267, 50268,   100,   173,    25,    10,  3254,     4,     2,   100,\n",
      "           524,    10,  2602, 37958,     4,     2,   100,   524,  2997,    19,\n",
      "            10,  1159,     4,     2,   100,   657,     7,  7142,     4,     2,\n",
      "           100,   101,  2600,     4,     2, 50265,  1711,    18,   205,     7,\n",
      "          1798,     4,     2, 50266,  2847,     6,    99,    18,   110,  2674,\n",
      "           689,     7,  7142,   116,     2, 50265,   100,   657,  6836, 18236,\n",
      "             4,    38,   146,    10,  1266, 33362,   741,  8982,  4977,   242,\n",
      "             4,     2, 50266,  7516,     6,    14,  4428, 10964,     4,    38,\n",
      "           657, 18236,   350,     4,     2, 50265,  5096,   350,     4,    38,\n",
      "           115,  3529,    24,   358,   183,     4,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [50267, 50268,   100,   657,     5,  1971,  6187,     4,     2, 10285,\n",
      "           688,   939,    21,    23,    10,   964,  3312,     4,     2,  2387,\n",
      "         21039,    16, 11267,  1521,     4,     2,   100,   173,    11,   647,\n",
      "             4,     2, 50265,  1185,   197,   328,    85,    18,    10,   269,\n",
      "           205,   527,    12,  9756,   177,     4,     2, 50266,   100,   581,\n",
      "            33,     7,  1649,    24,    66,     4,   653,    59,  4133,   116,\n",
      "           653,    18,   110,  2674,  1569,   116,     2, 50265,   100,   101,\n",
      "            10,   319,     9,   430,  4133,     6,    53,   127,  2674,    16,\n",
      "          1153,    20,  8390,  1193,  3153, 34279,     4,     2, 50266,  1711,\n",
      "            18,    10,   372,  1569,   328,    38,   657,     5,   527,     4,\n",
      "             2, 50265, 14783,     6,    24,    18,    10,   269,   205,  1569,\n",
      "             4,     2]])\n",
      "Decoder Input IDs : tensor([[50266,   100,   437,    45,   686,    38,   115,  3679,    14,     4,\n",
      "            38,   657, 18236,     6,    53,    38,   218,    75,   206,    38,\n",
      "           115,  3529,    24,   358,   183,     4,     2],\n",
      "        [50266,   243,    18,  2579,     7,   972,   951,    54,    34,  1122,\n",
      "          3168,     7,   162,     4,     2,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]])\n",
      "Labels: tensor([[50266,   100,   437,    45,   686,    38,   115,  3679,    14,     4,\n",
      "            38,   657, 18236,     6,    53,    38,   218,    75,   206,    38,\n",
      "           115,  3529,    24,   358,   183,     4,     2],\n",
      "        [50266,   243,    18,  2579,     7,   972,   951,    54,    34,  1122,\n",
      "          3168,     7,   162,     4,     2,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    # Move the batch to the device\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    decoder_input_ids = batch['decoder_input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    # Print the shapes of the tensors\n",
    "    print(f\"Input ID: {input_ids}\")\n",
    "    print(f\"Decoder Input IDs : {decoder_input_ids}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "Batch 10 | Loss: 4.2847 | Perplexity: 72.5843\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 20\u001b[0m\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     15\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m     16\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 1  # for example\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\")\n",
    "\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate perplexity for the current batch\n",
    "        perplexity = torch.exp(loss).item()\n",
    "\n",
    "        # Update progress bar description\n",
    "        progress_bar.set_postfix({\n",
    "            'Batch': batch_idx+1,\n",
    "            'Loss': f\"{loss.item():.4f}\",\n",
    "            'Perplexity': f\"{perplexity:.4f}\"\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} completed | Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e6bf846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I like to read books.\n"
     ]
    }
   ],
   "source": [
    "query = \"<latent><persona> I love hiking and books. <query> What's your hobby?\"\n",
    "inputs = tokenizer(query, return_tensors='pt').to(device)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    max_length=50,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Response:\", response)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmedr_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
