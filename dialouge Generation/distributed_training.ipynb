{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3127fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3da8aaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the BART model and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "add_special_tokens = {'additional_special_tokens': ['<query>', '<response>', '<latent>', '<persona>']}\n",
    "tokenizer.add_special_tokens(add_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a1b355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "   \"num_latent\": 10,\n",
    "   \"num_latent2\": 10,\n",
    "}\n",
    "\n",
    "encoded_tokens = tokenizer.encode(add_special_tokens['additional_special_tokens'])\n",
    "args['bos'], args[\"query\"], args[\"response\"], args[\"latent\"], args[\"persona\"], args[\"eos\"] = encoded_tokens\n",
    "args[\"pad\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5be94c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 GPUs\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "print(f\"Using {num_gpus} GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee822d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [50267, 50268, 100, 95, 2162, 10, 1518, 92, 790, 4, 2, 100, 101, 7, 3836, 23, 5, 950, 4, 2, 100, 422, 10, 2335, 41227, 334, 4, 2, 100, 33, 10, 380, 4045, 13495, 4, 2, 100, 101, 602, 8, 6016, 842, 462, 16731, 4, 2, 50265, 30086, 6, 38, 437, 646, 44518, 112, 18, 766, 8174, 653, 18, 110, 766, 116, 2], 'labels': [50266, 30086, 6, 38, 437, 646, 44518, 132, 18, 766, 8174, 85, 18, 2579, 7, 972, 47, 4, 2]}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load the pickle file\n",
    "with open('./Synthetic-Persona-Chat/prepared_data/test_data.pkl', 'rb') as f:\n",
    "    data_set = pickle.load(f)\n",
    "\n",
    "# Now `data_set` contains your loaded data\n",
    "print(data_set[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a037761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "# Convert to torch dataset\n",
    "class CNNDailyMailDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids = torch.tensor(item['input_ids'])\n",
    "        # attention_mask = torch.tensor(item['attention_mask'])\n",
    "        labels = torch.tensor(item['labels'])\n",
    "         \n",
    "\n",
    "        decoder_input_ids = labels\n",
    "        # labels = labels[1:]\n",
    "\n",
    "        # decoder_input_ids = labels[:-1]\n",
    "        # labels = labels[1:]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            # 'attention_mask': attention_mask,\n",
    "            'decoder_input_ids': decoder_input_ids,\n",
    "            'labels': labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5aa3ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CollateFn:\n",
    "    def __init__(self, pad_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        input_ids = [item['input_ids'] for item in batch]\n",
    "        decoder_input_ids = [item['decoder_input_ids'] for item in batch]\n",
    "        labels = [item['labels'] for item in batch]\n",
    "\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id)\n",
    "        decoder_input_ids = pad_sequence(decoder_input_ids, batch_first=True, padding_value=self.pad_token_id)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # ignore index\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'decoder_input_ids': decoder_input_ids,\n",
    "            'labels': labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02b292c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = args['pad']\n",
    "# Create the dataset and dataloader\n",
    "train_dataset = CNNDailyMailDataset(data_set)\n",
    "\n",
    "# Create a collate function to pad the sequences\n",
    "pad_fn = CollateFn(pad_token_id)\n",
    "sampler = RandomSampler(train_dataset)\n",
    "train_loader = DataLoader(train_dataset,batch_size=2,collate_fn=pad_fn,sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66c6506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID: tensor([[50267, 50268,   100,   524,    10,  9613,     4,     2,  2387,  2674,\n",
      "          1971,    16,   295,   853, 40949,     4,     2,   100,   101,   878,\n",
      "             4,     2,   100,   173,    23,  4716,  2793,     4,     2, 50265,\n",
      "           100,   101,     7,   213,     7,     5,  4105,     6,   310,   569,\n",
      "           426,     6,     8,  1166,     4,     2, 50266,   100,   101,     7,\n",
      "           213,     7,     5,  4105,   350,   328,    38,    67,   101,     7,\n",
      "           213,  5651,     8,  7358,     4,     2, 50265,  7516,     6,  3035,\n",
      "           328,    38,   657,  5651,   350,     4,     2, 50266,  5096,   350,\n",
      "           328,     2, 50265,  2847,     6,    99,   109,    47,   206,     9,\n",
      "          1261,    98,   444,   116,     2,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [50267, 50268,   100,   101,     7, 29716, 15146,    13,  7272,    11,\n",
      "             5,  1098,     4,     2,   100,   101,     7,  4161,     7,   247,\n",
      "           930,     4,     2,   100,   524,    10, 15848,     4,     2,   100,\n",
      "           524,  3276,     7,   173,    23,    10,   633,   142,     9,    10,\n",
      "         11096,     4,     2, 50265,   100,   657,    14,  1040,   328,    85,\n",
      "            21,    98, 31803,  2650,     4,     2, 50266,   100,   216,     6,\n",
      "           235,   116,    38,  1705,    75,   342,    24,   159,     4,     2,\n",
      "         50265,  5096,  5063,     4,   407,     6,   147,   222,    47,  1733,\n",
      "            62,   116,     2, 50266,   100,  2307,    62,    15,    10,   739,\n",
      "          3380,     4,     2, 50265,  1711,    18,  3035,   328,    38,   348,\n",
      "           460,   770,     7,   697,    15,    10,  3380,     4,     2]])\n",
      "Decoder Input IDs : tensor([[50266,   100,   657,    24,   328,    38,   437,    98,  7785,    38,\n",
      "          1410,   259,     4,     2,     1,     1,     1,     1],\n",
      "        [50266,   243,    21,    10,   319,     9,   173,     6,    53,    24,\n",
      "            21,    67,    10,   319,     9,  1531,     4,     2]])\n",
      "Labels: tensor([[50266,   100,   657,    24,   328,    38,   437,    98,  7785,    38,\n",
      "          1410,   259,     4,     2,  -100,  -100,  -100,  -100],\n",
      "        [50266,   243,    21,    10,   319,     9,   173,     6,    53,    24,\n",
      "            21,    67,    10,   319,     9,  1531,     4,     2]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    # Move the batch to the device\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    decoder_input_ids = batch['decoder_input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    # Print the shapes of the tensors\n",
    "    print(f\"Input ID: {input_ids}\")\n",
    "    print(f\"Decoder Input IDs : {decoder_input_ids}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "436d722a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartScaledWordEmbedding(50269, 768, padding_idx=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the updated tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "Batch 10 | Loss: 4.2847 | Perplexity: 72.5843\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 20\u001b[0m\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     15\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m     16\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 1  # for example\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\")\n",
    "\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        loss = outputs.loss.mean()  # Average loss over the batch\n",
    "        # loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate perplexity for the current batch\n",
    "        perplexity = torch.exp(loss).item()\n",
    "\n",
    "        # Update progress bar description\n",
    "        progress_bar.set_postfix({\n",
    "            'Batch': batch_idx+1,\n",
    "            'Loss': f\"{loss.item():.4f}\",\n",
    "            'Perplexity': f\"{perplexity:.4f}\"\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} completed | Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53326728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "save_path = \"gpt2-multi-gpu\" if num_gpus > 1 else \"gpt2-single-gpu\"\n",
    "model.module.save_pretrained(save_path) if num_gpus > 1 else model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model saved at {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e6bf846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I like to read books.\n"
     ]
    }
   ],
   "source": [
    "query = \"<latent><persona> I love hiking and books. <query> What's your hobby?\"\n",
    "inputs = tokenizer(query, return_tensors='pt').to(device)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    max_length=50,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Response:\", response)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmedr_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
